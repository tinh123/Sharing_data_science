{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Naive Bayes theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "na√Øve Bayes classifiers TESTING are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na√Øve) independence assumptions between the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In machine learning we are often interested in selecting the best hypothesis (h) given data (d).\n",
    "\n",
    "- In a classification problem, our hypothesis (h) may be the class to assign for a new data instance (d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes‚Äô Theorem provides a way that we can calculate the probability of a hypothesis given the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(h|d)= \\frac{(P(d|h) P(h))}{ P(d)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- P(h|d) = Posterior probability. The probability of hypothesis h being true, given the data d, where P(h|d)= P(d1| h) P(d2| h)‚Ä¶.P(dn| h) P(d)\n",
    "- P(d|h) = Likelihood. The probability of data d given that the hypothesis h was true.\n",
    "- P(h) = Class prior probability. The probability of hypothesis h being true (irrespective of the data)\n",
    "- P(d) = Predictor prior probability. Probability of the data (irrespective of the hypothesis)\n",
    "\n",
    "=> Conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the posterior probability for a number of different hypotheses, you can select the hypothesis with the highest probability. This is the maximum probable hypothesis and may formally be called the maximum a posteriori (MAP) hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifiers\n",
    "Naive Bayes classifiers are a collection of classification algorithms based on Bayes‚Äô Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes is a classification algorithm for binary (two-class) and multi-class classification problems. \n",
    "- The technique is easiest to understand when described using binary or categorical input values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with regards to our dataset, we can apply Bayes‚Äô theorem in following way:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y|X) = \\frac{P(X|y)P(y)}{P(X)}\n",
    "\\end{equation}\n",
    "\n",
    "where, y is class variable and X is a dependent feature vector (of size n) where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a set of independent variables:\n",
    "\\begin{equation}\n",
    "P(y|x_1,x_2,...,x_n) = \\frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P(y|x_1,x_2,...,x_n) = \\frac{P(y)\\prod_{i=1}^nP(x_i|y)}{ùëÉ(x_1)ùëÉ(x_2)...ùëÉ(x_ùëõ)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes model accuracy(in %): 95.0\n"
     ]
    }
   ],
   "source": [
    "# load the iris dataset \n",
    "from sklearn.datasets import load_iris \n",
    "iris = load_iris() \n",
    "  \n",
    "# store the feature matrix (X) and response vector (y) \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "  \n",
    "# splitting X and y into training and testing sets \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
    "  \n",
    "# training the model on training set \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "gnb = GaussianNB() \n",
    "gnb.fit(X_train, y_train) \n",
    "  \n",
    "# making predictions on the testing set \n",
    "y_pred = gnb.predict(X_test) \n",
    "  \n",
    "# comparing actual response values (y_test) with predicted response values (y_pred) \n",
    "from sklearn import metrics \n",
    "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
